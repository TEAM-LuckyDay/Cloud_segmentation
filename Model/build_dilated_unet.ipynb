{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUEX1yT6ObHp"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROaksixkOV2l"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import natsort\n",
        "import cv2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from tensorflow.keras.utils import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2LOYbAPOkwt"
      },
      "source": [
        "# Data Loder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IXRaHPoPOa-"
      },
      "outputs": [],
      "source": [
        "def sortlist(filelist):\n",
        "    filelist = natsort.natsorted(filelist)\n",
        "    return filelist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SZpwghSOfP7"
      },
      "outputs": [],
      "source": [
        "dir_path = os.getenv(\"HOME\")+ '/Cloud_data/cloud_train/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfIKSPWLPt9O"
      },
      "outputs": [],
      "source": [
        "class CloudGenerator(Sequence):\n",
        "    def __init__(self,\n",
        "                 dir_path,\n",
        "                 batch_size = 4,\n",
        "                 img_size = (800,800,3),\n",
        "                 output_size = (800,800),\n",
        "                 is_train = True):\n",
        "        \n",
        "        self.dir_path = dir_path\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.output_size = output_size\n",
        "        self.is_train = is_train\n",
        "\n",
        "        self.data = self.load_dataset()\n",
        "    \n",
        "    def load_dataset(self):\n",
        "        input_images = glob(os.path.join(self.dir_path,\"patch_img\",\"*png\"))\n",
        "        label_images = glob(os.path.join(self.dir_path,\"patch_labeling\",\"*png\"))\n",
        "        input_images = sortlist(input_images)\n",
        "        label_images = sortlist(label_images)\n",
        "        \n",
        "        assert len(input_images) == len(label_images)\n",
        "        data = [ _ for _ in zip(input_images, label_images)]\n",
        "        \n",
        "        train_percent = int(len(data) * 0.8)\n",
        "\n",
        "        if self.is_train:\n",
        "            return data[:train_percent]\n",
        "        return data[train_percent:]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_data = self.data[\n",
        "                           index*self.batch_size:\n",
        "                           (index + 1)*self.batch_size\n",
        "                           ]\n",
        "        inputs = np.zeros([self.batch_size, *self.img_size])\n",
        "        outputs = np.zeros([self.batch_size, *self.output_size])\n",
        "            \n",
        "        for i, data in enumerate(batch_data):\n",
        "            input_img_path, output_path = data\n",
        "            _input = cv2.imread(input_img_path)\n",
        "            _output = cv2.imread(output_path,0)\n",
        "            _output = (_output==50).astype(np.uint8)*1\n",
        "            inputs[i] = _input/255\n",
        "            outputs[i] = _output\n",
        "            \n",
        "            return inputs, outputs\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.data) / self.batch_size)\n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.arange(len(self.data))\n",
        "        if self.is_train == True :\n",
        "            np.random.shuffle(self.indexes)\n",
        "            return self.indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVaZ_wfrTjqT"
      },
      "outputs": [],
      "source": [
        "train_generator = CloudGenerator(\n",
        "    dir_path,\n",
        "    is_train=True\n",
        ")\n",
        "\n",
        "test_generator = CloudGenerator(\n",
        "    dir_path,\n",
        "    is_train=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Atrous U-Net Model"
      ],
      "metadata": {
        "id": "KM2aLzxV-zL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def atrous_unet_encoder(x, channel = 64, kernel_size = (3,3), strides=1,activation='relu'):\n",
        "    skip_connection = []\n",
        "    for i in range(4):\n",
        "        x = Conv2D(channel* 2**i, kernel_size, activation=activation, padding='same',kernel_initializer='he_normal')(x)\n",
        "        x = Conv2D(channel* 2**i, kernel_size, activation=activation, padding='same',kernel_initializer='he_normal')(x)\n",
        "        skip_connection.append(x)\n",
        "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    return x, skip_connection\n",
        "\n",
        "def atrous_unet_bottleneck(x,kernel_size = (3,3),activation='relu'):\n",
        "    for i in range(6):\n",
        "        x = Conv2D(1024, kernel_size, activation=activation, padding='same',dilation_rate=(2**i, 2**i),kernel_initializer='he_normal')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    return x\n",
        "\n",
        "def atrous_unet_decoder(x,skip_connection,channel = 64, kernel_size = (3,3),activation = 'relu'):\n",
        "    for i in reversed(range(4)):\n",
        "        x = Conv2DTranspose(channel* 2**i, 2, strides = (2,2), activation=activation,kernel_initializer='he_normal')(x)\n",
        "        x = concatenate([skip_connection[i],x],axis =3)\n",
        "        x = Conv2D(channel* 2**i, kernel_size, activation=activation, padding='same',kernel_initializer='he_normal')(x)\n",
        "        x = Conv2D(channel* 2**i, kernel_size, activation=activation, padding='same',kernel_initializer='he_normal')(x)\n",
        "    return x\n",
        "\n",
        "def atrous_unet_model(input_shape=(800,800,3)):\n",
        "    input = Input(input_shape)\n",
        "    encoder,skip = atrous_unet_encoder(input)\n",
        "    bottleneck = atrous_unet_bottleneck(encoder)\n",
        "    decoder = atrous_unet_decoder(bottleneck,skip)\n",
        "    output = Conv2D(1,1,activation='sigmoid')(decoder)\n",
        "    Atrous_unet_model = Model(inputs = input,outputs = output)\n",
        "    return Atrous_unet_model\n",
        "    \n",
        "Atrous_unet_model = atrous_unet_model()\n",
        "Atrous_unet_model.summary()"
      ],
      "metadata": {
        "id": "fZnxJOOyYn9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Atrous_unet_model_path = os.getenv(\"HOME\")+'/Cloud_data/cloud_model/seg_atrous_unet_model.h5'\n",
        "\n",
        "# Atrous_unet_model = atrous_unet_model()\n",
        "Atrous_unet_model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy')\n",
        "Atrous_unet_model.fit_generator(\n",
        "     generator=train_generator,\n",
        "     validation_data=test_generator,\n",
        "     steps_per_epoch=len(train_generator),\n",
        "     epochs=10,\n",
        " )\n",
        "\n",
        "Atrous_unet_model.save(Atrous_unet_model_path)  #학습한 모델을 저장해 주세요."
      ],
      "metadata": {
        "id": "j-RnIAJbY114"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "atrous_unet_model_path = '/content/drive/MyDrive/Cloud_data/cloud_model/seg_atrous_unet_model.h5'"
      ],
      "metadata": {
        "id": "eLhn-ZN8_1SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Atrous_unet_model = tf.keras.models.load_model(atrous_unet_model_path)"
      ],
      "metadata": {
        "id": "2h1Ta3iF_07J"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JUEX1yT6ObHp",
        "E2LOYbAPOkwt",
        "KM2aLzxV-zL2"
      ],
      "name": "build_dilated_unet.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}